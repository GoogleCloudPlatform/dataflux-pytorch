The code in this directory trains the Llama 7B model on [Huggingface's RedPajama dataset](https://huggingface.co/datasets/togethercomputer/RedPajama-Data-1T-Sample/tree/main). All the code in this directory is borrowed from [Lightning AI's lit-llama repo](https://github.com/Lightning-AI/lit-llama). Changes have been made in appropriate places to fetch the data from GCS instead of reading from disk.

This demo has been tested on a GCE instance with `2` Nvidia `H100` GPUs.

### Download the data

Follow the instructions [here](https://github.com/Lightning-AI/lit-llama/blob/main/howto/train_redpajama.md#prepare-redpajama-for-training) to download the dataset. Upload the directory with `.bin` files generated by the pre-processing script to your GCS bucket by running the command below

```
gcloud storage cp -r /path/to/your/local/dir gs://<YOUR-BUCKET>/
```


### Run the pre-training script

Changes have been made to `lit-llama`'s training code at appropriate places to leverage GCS connector for Pytorch to quickly and efficiently perform data listing, data loading, and checkopint saving. 

A custom Fully Sharded Data Parallel (FSDP) strategy that inherits the `lightning.fabric.strategies.FSDPStrategy` (see `strategies.py`) has been implemented to save model checkpoints using `dataflux_pytorch.lightning.DatafluxLightningCheckpoint`. 

```
# pretrain.py; to list all the files in the dataset.
def list_with_dataflux(project_name, bucket_name):
    dataset = dataflux_iterable_dataset.DataFluxIterableDataset(
        project_name=project_name, bucket_name=bucket_name)
    filenames = [name for name, _ in dataset.objects]
    return filenames

```

```
# dataset.py; to download the `.bin` file to memory as bytes.
def _read(self, path):
    bytes_content = download_single(self.storage_client, self.bucket_name,
                                    path)
    bytes_io = io.BytesIO(bytes_content)
    magic = bytes_io.read(len(HDR_MAGIC))
    assert magic == HDR_MAGIC, "File doesn't match expected format."
    version = struct.unpack("<Q", bytes_io.read(8))
    assert (1, ) == version
    (dtype_code, ) = struct.unpack("<B", bytes_io.read(1))
    dtype = dtypes[dtype_code]
    (chunk_size, ) = struct.unpack("<Q", bytes_io.read(8))
    return dtype, chunk_size, bytes_io
```

Update the `bucket_name` and `project_name` variables in `pretrain.py` and `dataset.py`. Then, from the root of the repo, run

```
python3 demo.llama.train --devices 2
```

### Download the tokenizer

Follow the instructions [here](https://github.com/Lightning-AI/lit-llama/blob/main/howto/download_weights.md#openllama) to download the Llama Tokenzier. This is needed to test the model you just trained.

### Submit your prompt to the model and get tokens back!

Run `generate.py` to get a response back from the model

```
you@your-machine:~/dataflux-pytorch/demo/llama$ python generate.py --prompt "Hello, my name is" --checkpoint_path out/training/iter-000119-ckpt.pth --tokenizer_path path/to/tokenizer
Loading model ...
Time to load model: 4.67 seconds.
Seed set to 1234
Hello, my name is.. a
 live the
 at,,.0,1);, in as may. already seems never . of,, and andure to the five a3, out.: and.er1,, to.If included
Time for inference 1: 1.17 sec total, 42.78 tokens/sec
Memory used: 13.57 GB
```

